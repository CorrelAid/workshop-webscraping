---
title: "01session-solutions"
author: "Zoé Wolter & Philipp Bosch"
date: "Oct 21, 2022"
output: html_document
---

# AGENDA

- Intro: Was ist Web Scraping überhaupt?
- Einführung in das Projekt für den Workshop
- Download von HTMLs
- Extrahieren von Informationen aus den HTMLs

# Intro: Was ist Web Scraping überhaupt?
Bei Web Scraping geht es um das Sammeln von Informationen von Websites, indem man diese direkt aus dem HTML Source Code extrahiert. Warum denn aber?

- Daten über Daten
- kein Copy&Paste mehr
- Automatisierung der Datensammlung
- reproduzierbare und aktualisierbare Datensammlung

# Unser Ziel
Heute:
  - Den Aufbau von HTMLs verstehen
  - Extraktion von Informationen anhand ihrer XPaths
  - Anwenden auf cyberLAGO-Event-Homepage


# Packages laden

Zuerst (installieren und) laden wir alle Packages, die wir heute und am zweiten Workshop-Tag benötigen: 
```{r}
source(knitr::purl("packages.Rmd", quiet = TRUE))
```


# Events Homepage

## URL

Zuerst suchen wir uns die URL, von der wir uns die Daten ziehen wollen. Diese speichern wir für die weitere Bearbeitung in einem Vektor. Dafür speichern wir entweder die Base URL (Stamm-URL), von der aus man weitere URLs zusammenbauen kann oder direkt die gesamte URL:

```{r}
base_url <- 'https://cyberlago.net/'
events_url <- 'https://cyberlago.net/events/'
```

## Be polite

Bevor wir mit dem eigentlichen Scraping beginnen, sollten wir der Website einige Informationen über uns durch den User-Agent übergeben und uns über mögliche Einschränkungen durch die Website informieren. Dafür gibt es verschiedene Möglichkeiten, sehr einfach geht es beispielsweise mit dem `polite`-Package:

```{r}
polite::bow(url = str_c(base_url), 
            user_agent = 'Workshop Web Data Collection - zoe.w@correlaid.org') -> session
```

Hier bekommen wir zwei sehr wichtige Informationen: 

- Crawl-Delay: 5 Sekunden
- Wir dürfen scrapen!

## Scraping der Website: htmls downloaden

Nachdem wir die Website über uns informiert haben und wir wissen, dass wir die cyberLAGO-Seite auch scrapen dürfen, können wir damit jetzt endlich loslegen: Wir laden uns die html-Seite (als Liste von <head> und <body>) in R!

```{r}
session %>% 
  polite::nod(str_c('events')) %>%
  polite::scrape() -> events_html
```

Da sich - besonders bei Events-Seiten - sich Inhalt und html-Struktur zeitnah verändern können, ist es best practice, die html-Dateien zu downloaden und zu speichern. So läuft unser Code auch noch, wenn die Seite sich online schon wieder verändert hat. 

```{r}
# create directory to store the htmls
if (!dir.exists(here::here('assets', 'htmls'))) {
  dir.create(here::here('assets', 'htmls'))
}

# function to download htmls
download_html <- function(url, filename) {
  polite::nod(session, url) %>%
    polite::rip(destfile = filename,
                path = here::here('assets', 'htmls'),
                overwrite = TRUE)
  }

# call function to download html 
download_html(str_c(base_url, 'events'), 'events.html')
```

Hinweis: So wahnsinnig viel Sinn ergibt es hier nicht, eine Funktion für den Download zu schreiben. Die bekommt erst dadurch so richtig Power, wenn wir sie auf eine Liste an URLs anwenden mit `purrr::map()`!


## XPath: Daten extrahieren

Jetzt haben wir zwar die html, so irre viel können wir bisher aber leider auch noch nicht damit anfangen... Wir brauchen nur die für uns relevanten Daten, hier die Tabelle. Um an die Tabelle zu kommen, können wir mit **XPath** arbeiten! Da XPath zwar sehr nützlich, aber genauso nervig sein kann, gibt es Hilfmittel, damit wir die Pfade nicht selbst basteln müssen: 

- Rechtsklick > Untersuchen/Inspect > HTML Node suchen
- [Selector Gadget](https://selectorgadget.com/): "SelectorGadget is an open source tool that makes CSS selector generation and discovery on complicated sites a breeze"

Hier die XPaths zu den Tabellen, die uns hier interessieren:
- *Titel*: /html/body/div[1]/div[2]/div[1]/div[2]/div/div/div/div/div/div[3]/div/div[2]/div[3]/div[2]/ul/li[4]/div/div[2]/h3/a
- *Datum*: /html/body/div[1]/div[2]/div[1]/div[2]/div/div/div/div/div/div[3]/div/div[2]/div[3]/div[2]/ul/li[4]/div/div[2]/div[1]/div/div[1]/span[1]
- *Location*: /html/body/div[1]/div[2]/div[1]/div[2]/div/div/div/div/div/div[3]/div/div[2]/div[3]/div[2]/ul/li[4]/div/div[2]/div[1]/div/div[2]

Wir wollen mal den Titel der heutigen Veranstaltung extrahieren:
```{r}
events_html %>%
  rvest::html_element(xpath = '//ul/li[1]/div/div[2]/h3/a') %>% 
  rvest::html_text()
```
Das geht auch für eine der anderen Veranstaltungen! Wo ist der Unterschied im XPath?
```{r}
events_html %>%
  rvest::html_element(xpath = '//ul/li[2]/div/div[2]/h3/a') %>% 
  rvest::html_text()
```

```{r}
events_html %>%
  rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/h3/a') %>% 
  rvest::html_text() -> event_titles
```

Das geht jetzt natürlich nicht nur für die Titel, wir wollen auch noch Datum, Uhrzeit und Ort, sowie den hinterlegten Event-Link für jede der angezeigten Veranstaltungen:
```{r}
# Titel
events_html %>%
  rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/h3/a') %>% 
  rvest::html_text()

# Datum & Zeit Beginn
events_html %>%
  rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/div[1]/div/div[1]/span[1]') %>% 
  rvest::html_text()

# Zeit Ende
events_html %>%
  rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/div[1]/div/div[1]/span[2]') %>% 
  rvest::html_text()

# Location
events_html %>%
  rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/div[1]/div/div[2]') %>% 
  rvest::html_text()

#URL
events_html %>% 
  rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/h3/a') %>% 
  rvest::html_attr('href')
```


## Datenbereinigung
Inzwischen haben wir zwar die Teile der html-Seite, die uns interessieren - aber noch nicht in einer brauchbaren Form für weitere Analysen. Daher once again: Datenbereinigung!
Wir ziehen uns alle extrahierten Daten zusammen in einen Datensatz und erledigen dabei direkt die hier überschaubare Bereinigung:
```{r}
data.frame(
  event = events_html %>% rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/h3/a') %>% rvest::html_text2(),
  date = sub(' ,.*', '', events_html %>% rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/div[1]/div/div[1]/span[1]') %>% rvest::html_text2()),
  start = sub('.*, ', '', events_html %>% rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/div[1]/div/div[1]/span[1]') %>% rvest::html_text2()),
  end = events_html %>% rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/div[1]/div/div[1]/span[2]') %>% rvest::html_text2(),
  location = gsub('^\\|', '', events_html %>% rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/div[1]/div/div[2]') %>% rvest::html_text2()),
  url = events_html %>% rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/h3/a') %>% rvest::html_attr('href')
) -> df # @Phil: äm ja schön ist anders, Ideen?
```

Und schon haben wir die Daten! Eigentlich gar nicht so viel Code, oder?

## Datensatz speichern
Und schön ist der Datensatz inzwischen und soll daher auch gespeichert werden - wir wollen das Skript fürs Scrapen ja nicht vor jeder Analyse durchlaufen lassen müssen, sondern können dann direkt einfach den Datensatz laden und haben dann auch keine Probleme, falls sich die Website verändert und unser wunderbares Scraping-Skript nicht mehr durchlaufen will...

```{r}
saveRDS(df, file = here::here('data', 'events.RDS'))
```

# Hands-On
tba

# Scraping at Scale
So jetzt haben wir die Daten der aktuellen Events, dann lasst uns doch einmal träumen... Was wäre wenn wir das automatisiert für zurückliegende Events machen könnten?! Here we go:

Wir schauen uns erstmal einige URLs der Websites mit den vergangenen Events an:
- https://cyberlago.net/events/liste/?tribe_event_display=past&tribe_paged=1
- https://cyberlago.net/events/liste/?tribe_event_display=past&tribe_paged=2
... und das geht 33 Seiten in genau dieser Struktur!

```{r}
# Base URL definieren
base_url <- 'https://cyberlago.net/'

# Be polite
session <- polite::bow(url = base_url, 
                       user_agent = 'Workshop Web Data Collection - zoe.w@correlaid.org')

# Vektor mit allen Seiten, die wir uns anschauen wollen
pages <- 1:33

# Scrapen und Downloaden aller HTML Dateien von diesen Seiten
# Mit purrr können wir über den Vektor mit den Seiten-Zahlen gehen...
purrr::map(.x = pages, ~ {
  #...die URL für jedes Jahr zusammenbasteln und uns jeweils auf der Seite anmelden...
  polite::nod(session, str_c('events//liste/?tribe_event_display=past&tribe_paged=', .x)) %>% 
    #...die HTML scrapen...
    polite::scrape()
  #...und abspeichern!
}) -> results   
# @Phil: vielleicht noch einbauen, dass die auch lokal gespeichert werden?
```

```{r}
all_events <- setNames(data.frame(matrix(ncol = 6, nrow = 0)), c('event', 'date', 'start', 'end', 'location', 'url'))

for (i in 1:length(results)) {
  print(i)
  data.frame(
    event = results[[i]] %>% rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/h3/a') %>% rvest::html_text2(),
    date = sub(' ,.*', '', results[[i]] %>% rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/div[1]/div/div[1]/span[1]') %>% rvest::html_text2()),
    start = sub('.*, ', '', results[[i]] %>% rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/div[1]/div/div[1]/span[1]') %>% rvest::html_text2()),
    end = results[[i]] %>% rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/div[1]/div/div[1]/span[2]') %>% rvest::html_text2(),
    location = gsub('^\\|', '', results[[i]] %>% rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/div[1]/div/div[2]') %>% rvest::html_text2()),
    url = results[[i]] %>% rvest::html_nodes(xpath = '//ul/li[*]/div/div[2]/h3/a') %>% rvest::html_attr('href')
  ) -> tmp
  all_events <- rbind(test, tmp)
} 
# @Phil: Da gibt's doch sicher auch eine charmantere purrr-Lösung zu? 
# @Phil: Bei 26, 28, 29 crasht das ganze, da passen irgendwelche Längen nicht zusammen, kannst du das vielleicht mal checken?
```

Der Datensatz ist zwar schon erstaunlich clean, ein bisschen Aufwand müssten wir jetzt aber eigentlich noch in die Bereinigung stecken - sparen wir uns jetzt für den Moment. Trotzdem wichtig: speichern des Datensatzes!

```{r}
saveRDS(all_events, file = here::here('data', 'all_events.RDS'))
```
